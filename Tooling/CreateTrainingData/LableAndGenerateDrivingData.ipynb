{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "widthSteering = heightSteering = 140\n",
    "\n",
    "widthRoad = 600\n",
    "heightRoad = 200\n",
    "\n",
    "modelsPath = 'Models'\n",
    "modelName = 'bestModel.torch'\n",
    "pathToModel = f'{modelsPath}/{modelName}'\n",
    "\n",
    "framesPath = 'DrivingFramesStarting'\n",
    "labeledFrames = 'LabledFrames'\n",
    "\n",
    "\n",
    "#road crop area\n",
    "x1R = 400\n",
    "x2R = 600\n",
    "y1R = 630\n",
    "y2R = 1280\n",
    "\n",
    "#road image size\n",
    "widthRoad  = y2R - y1R\n",
    "heightRoad = x2R - x1R\n",
    "\n",
    "#wheel crop area\n",
    "x1W = 812\n",
    "x2W = 932\n",
    "y1W = 913\n",
    "y2W = 1033\n",
    "\n",
    "#wheel image size\n",
    "widthWheel  = y2W - y1W\n",
    "heightWheel = x2W-x1W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Directoty Exists\n",
      "Starting Frames Directoty Exists\n",
      "Labled Frames Directory Exists\n"
     ]
    }
   ],
   "source": [
    "exist = os.path.exists(modelsPath)\n",
    "if exist:\n",
    "    print('Models Directoty Exists')\n",
    "else:\n",
    "    print('Models Directory Does not Exists')\n",
    "\n",
    "exist = os.path.exists(framesPath)\n",
    "if exist:\n",
    "    print('Starting Frames Directoty Exists')\n",
    "else:\n",
    "    print('Starting Frames Directory Does not Exists')\n",
    "\n",
    "\n",
    "exist = os.path.exists(labeledFrames)\n",
    "if not exist:\n",
    "    os.makedirs(labeledFrames)\n",
    "    print(\"Labled Frames Directoty Created\")\n",
    "else:\n",
    "    print('Labled Frames Directory Exists')\n",
    "    #Uncomment these two lines if you want to delete the frames folder before adding more frames\n",
    "    #shutil.rmtree(labeledFrames)\n",
    "    #os.makedirs(labeledFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingFrames = [f for f in listdir(framesPath) if isfile(join(framesPath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(startingFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.fc = nn.Linear(512, 128)\n",
    "        \n",
    "        self.branch_a1 = nn.Linear(128, 32)\n",
    "        self.branch_a2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc(x))\n",
    "\n",
    "        a = F.leaky_relu(self.branch_a1(x))\n",
    "        \n",
    "        out1 = self.branch_a2(a)\n",
    "        \n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "#load net\n",
    "#To Load Pretrained Weights:   weights='ResNet18_Weights.DEFAULT'\n",
    "resnet18 = torchvision.models.resnet18()\n",
    "resnet18.fc = nn.Identity()\n",
    "net_add=net()\n",
    "model = nn.Sequential(resnet18, net_add)\n",
    "# Set device GPU or CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "# load model to GPU\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(pathToModel))\n",
    "model.eval()\n",
    "\n",
    "count = 0\n",
    "for file in startingFrames:\n",
    "    noExtension = file[:-4]\n",
    "    filePath = f'{framesPath}/{file}'\n",
    "\n",
    "\n",
    "    fileSize = os.path.getsize(filePath)\n",
    "    if fileSize>100:\n",
    "        #load image\n",
    "        wholeFrame = cv2.imread(filePath)\n",
    "        \n",
    "\n",
    "        #wheel angle prediction\n",
    "        crop = wholeFrame[x1W:x2W, y1W:y2W]\n",
    "        grayImage = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        (thresh, contrastImg) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)\n",
    "        backtorgb = cv2.cvtColor(contrastImg,cv2.COLOR_GRAY2RGB)\n",
    "        transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((heightWheel,widthWheel)),tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) \n",
    "        wheel=transformImg(np.array(backtorgb))\n",
    "        wheel = wheel.to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(wheel)\n",
    "        detectedAngle = prediction.data.cpu().numpy()[0][0]\n",
    "        realAngle = (180*detectedAngle)-90\n",
    "        \n",
    "\n",
    "        # crop to road\n",
    "        crop = wholeFrame[x1R:x2R, y1R:y2R]\n",
    "\n",
    "        # resize to 180*56\n",
    "        resized = cv2.resize(crop, [180,56], interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "        cv2.imwrite(f'{labeledFrames}/{noExtension}_roadcrop_alpha_none_filter_none_angleNorm_{detectedAngle}.jpg', resized)\n",
    "\n",
    "        #array of all angle values to adjust brightness by\n",
    "        #alphas = [0.2,0.4,0.6,0.8,1.2,1.4]\n",
    "        \n",
    "        #for alpha in alphas:\n",
    "\n",
    "                #brighness = cv2.convertScaleAbs(resized, alpha, alpha)\n",
    "                #cv2.imwrite(f'{labeledFrames}/{noExtension}_roadcrop_alpha_{alpha}_filter_none_angleNorm_{detectedAngle}.jpg', brighness)\n",
    "\n",
    "                #for f in range(2,3):\n",
    "                   # blur = cv2.blur(brighness,(f,f))\n",
    "\n",
    "                    #cv2.imwrite(f'{labeledFrames}/{noExtension}_roadcrop_alpha_{alpha}_filter_{f}_angleNorm_{detectedAngle}.jpg', blur)\n",
    "    count = count + 1\n",
    "    if count%1000==0:\n",
    "         print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentedFrames = [f for f in listdir(labeledFrames) if isfile(join(labeledFrames, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485431"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmentedFrames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
